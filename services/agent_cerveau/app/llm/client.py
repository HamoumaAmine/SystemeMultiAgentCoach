"""
Client LLM pour l'agent cerveau.

Pour l'instant, c'est un simple placeholder.
Plus tard, tu ajouteras ici :
 - LangChain
 - ton modèle (API ou local)
 - la configuration des prompts
"""


class LLMClient:
    def __init__(self):
        # Exemple : initialisation du modèle, API key, etc.
        pass

    def generate(self, prompt: str) -> str:
        """
        Génère une réponse textuelle à partir d'un prompt.
        MVP : retour fixe.
        """
        return "Réponse du LLM (placeholder, à implémenter plus tard)."
